{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "task1-header"
   },
   "source": [
    "# Task 1: RAG Model for Business QA Bot\n",
    "\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) system for business question-answering using OpenAI API and Pinecone vector database.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "The RAG system consists of four main components:\n",
    "1. **Document Processor**: Cleans and chunks business documents\n",
    "2. **Vector Store**: Stores embeddings in Pinecone for semantic search\n",
    "3. **RAG System**: Orchestrates the entire pipeline\n",
    "4. **Streamlit Interface**: Provides user-friendly web interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai pinecone-client streamlit nltk numpy python-dotenv\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# AI and ML libraries\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api-setup"
   },
   "source": [
    "## API Configuration\n",
    "\n",
    "Set up your OpenAI and Pinecone API keys. You can get these from:\n",
    "- OpenAI: https://platform.openai.com/api-keys\n",
    "- Pinecone: https://app.pinecone.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api-keys"
   },
   "outputs": [],
   "source": [
    "# Set your API keys here\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"\n",
    "PINECONE_API_KEY = \"your-pinecone-api-key-here\"\n",
    "\n",
    "# Or load from environment variables\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "document-processor"
   },
   "source": [
    "## Document Processor\n",
    "\n",
    "The Document Processor handles text cleaning, normalization, and chunking for optimal embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "document-processor-class"
   },
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Document processor for preparing business documents for RAG pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize document processor\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Maximum size of each chunk in characters\n",
    "            chunk_overlap: Overlap between chunks in characters\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text to clean\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Remove special characters but keep punctuation for sentence structure\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', '', text)\n",
    "        \n",
    "        # Remove multiple consecutive punctuation\n",
    "        text = re.sub(r'([\\.!?]){2,}', r'\\1', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def create_chunks(self, text: str, source: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Create overlapping text chunks for better context preservation\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            source: Source document name\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks with metadata\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        text_length = len(text)\n",
    "        \n",
    "        # If text is shorter than chunk size, return as single chunk\n",
    "        if text_length <= self.chunk_size:\n",
    "            return [{\n",
    "                'content': text,\n",
    "                'source': source,\n",
    "                'start_char': 0,\n",
    "                'end_char': text_length\n",
    "            }]\n",
    "        \n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start < text_length:\n",
    "            # Calculate end position\n",
    "            end = min(start + self.chunk_size, text_length)\n",
    "            \n",
    "            # Try to break at sentence boundary if possible\n",
    "            if end < text_length:\n",
    "                sentence_end = text.rfind('.', start, end)\n",
    "                if sentence_end > start + self.chunk_size - 200:\n",
    "                    end = sentence_end + 1\n",
    "            \n",
    "            chunk_text = text[start:end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append({\n",
    "                    'content': chunk_text,\n",
    "                    'source': source,\n",
    "                    'start_char': start,\n",
    "                    'end_char': end,\n",
    "                    'chunk_index': chunk_index\n",
    "                })\n",
    "                chunk_index += 1\n",
    "            \n",
    "            # Move start position with overlap\n",
    "            start = end - self.chunk_overlap\n",
    "            \n",
    "            # Ensure we don't go backwards\n",
    "            if start <= 0:\n",
    "                start = end\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_document(self, content: str, source: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Main document processing pipeline\n",
    "        \n",
    "        Args:\n",
    "            content: Document content\n",
    "            source: Document source/filename\n",
    "            \n",
    "        Returns:\n",
    "            List of processed chunks ready for embedding\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Clean the text\n",
    "            cleaned_content = self.clean_text(content)\n",
    "            \n",
    "            if not cleaned_content.strip():\n",
    "                logger.warning(f\"Empty content after cleaning for document: {source}\")\n",
    "                return []\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = self.create_chunks(cleaned_content, source)\n",
    "            \n",
    "            logger.info(f\"Processed document {source} into {len(chunks)} chunks\")\n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing document {source}: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vector-store"
   },
   "source": [
    "## Vector Store\n",
    "\n",
    "The Vector Store manages Pinecone operations for storing and retrieving document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vector-store-class"
   },
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Vector store implementation using Pinecone for semantic search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, environment: str = \"us-east-1\", index_name: str = \"business-rag-index\"):\n",
    "        \"\"\"\n",
    "        Initialize Pinecone vector store\n",
    "        \n",
    "        Args:\n",
    "            api_key: Pinecone API key\n",
    "            environment: Pinecone environment\n",
    "            index_name: Name of the index to use\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.environment = environment\n",
    "        self.index_name = index_name\n",
    "        self.dimension = 1536  # OpenAI ada-002 embedding dimension\n",
    "        self.metric = \"cosine\"\n",
    "        self.pc = None\n",
    "        self.index = None\n",
    "        \n",
    "    def initialize_pinecone(self):\n",
    "        \"\"\"Initialize Pinecone client\"\"\"\n",
    "        try:\n",
    "            self.pc = Pinecone(api_key=self.api_key)\n",
    "            logger.info(\"Pinecone client initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Pinecone client: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_index(self):\n",
    "        \"\"\"Create Pinecone index if it doesn't exist\"\"\"\n",
    "        try:\n",
    "            if not self.pc:\n",
    "                self.initialize_pinecone()\n",
    "            \n",
    "            # Check if index exists\n",
    "            existing_indexes = self.pc.list_indexes()\n",
    "            index_names = [index.name for index in existing_indexes]\n",
    "            \n",
    "            if self.index_name not in index_names:\n",
    "                logger.info(f\"Creating new index: {self.index_name}\")\n",
    "                \n",
    "                # Create index with serverless spec\n",
    "                self.pc.create_index(\n",
    "                    name=self.index_name,\n",
    "                    dimension=self.dimension,\n",
    "                    metric=self.metric,\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud=\"aws\",\n",
    "                        region=self.environment\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Wait for index to be ready\n",
    "                while not self.pc.describe_index(self.index_name).status['ready']:\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                logger.info(f\"Index {self.index_name} created and ready\")\n",
    "            else:\n",
    "                logger.info(f\"Index {self.index_name} already exists\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating index: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def connect_to_index(self):\n",
    "        \"\"\"Connect to existing index\"\"\"\n",
    "        try:\n",
    "            if not self.pc:\n",
    "                self.initialize_pinecone()\n",
    "            \n",
    "            self.index = self.pc.Index(self.index_name)\n",
    "            logger.info(f\"Connected to index: {self.index_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error connecting to index: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def initialize_index(self):\n",
    "        \"\"\"Initialize the complete index setup\"\"\"\n",
    "        try:\n",
    "            self.initialize_pinecone()\n",
    "            self.create_index()\n",
    "            self.connect_to_index()\n",
    "            logger.info(\"Vector store initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def upsert_vectors(self, vectors: List[Dict[str, Any]], batch_size: int = 100):\n",
    "        \"\"\"\n",
    "        Upsert vectors to Pinecone index\n",
    "        \n",
    "        Args:\n",
    "            vectors: List of vectors with id, values, and metadata\n",
    "            batch_size: Number of vectors to process in each batch\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.index:\n",
    "                raise ValueError(\"Index not initialized. Call initialize_index() first.\")\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, len(vectors), batch_size):\n",
    "                batch = vectors[i:i + batch_size]\n",
    "                \n",
    "                # Format vectors for Pinecone\n",
    "                formatted_vectors = []\n",
    "                for vector in batch:\n",
    "                    formatted_vectors.append({\n",
    "                        'id': vector['id'],\n",
    "                        'values': vector['values'],\n",
    "                        'metadata': vector['metadata']\n",
    "                    })\n",
    "                \n",
    "                # Upsert batch\n",
    "                self.index.upsert(vectors=formatted_vectors)\n",
    "                logger.info(f\"Upserted batch {i // batch_size + 1} with {len(batch)} vectors\")\n",
    "            \n",
    "            logger.info(f\"Successfully upserted {len(vectors)} vectors\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error upserting vectors: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def query_vectors(self, query_vector: List[float], top_k: int = 5, \n",
    "                     include_metadata: bool = True, filter_dict: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query vectors from Pinecone index\n",
    "        \n",
    "        Args:\n",
    "            query_vector: Query embedding vector\n",
    "            top_k: Number of top results to return\n",
    "            include_metadata: Whether to include metadata in results\n",
    "            filter_dict: Optional metadata filter\n",
    "            \n",
    "        Returns:\n",
    "            Query results from Pinecone\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.index:\n",
    "                raise ValueError(\"Index not initialized. Call initialize_index() first.\")\n",
    "            \n",
    "            # Perform query\n",
    "            results = self.index.query(\n",
    "                vector=query_vector,\n",
    "                top_k=top_k,\n",
    "                include_metadata=include_metadata,\n",
    "                filter=filter_dict\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Query returned {len(results['matches'])} results\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error querying vectors: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rag-system"
   },
   "source": [
    "## RAG System\n",
    "\n",
    "The main RAG system that orchestrates document processing, embedding generation, and query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rag-system-class"
   },
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Main RAG system that orchestrates document processing, embedding generation,\n",
    "    vector storage, and query processing for business QA\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str, vector_store: VectorStore, document_processor: DocumentProcessor):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key\n",
    "            vector_store: Vector store instance\n",
    "            document_processor: Document processor instance\n",
    "        \"\"\"\n",
    "        self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        self.vector_store = vector_store\n",
    "        self.document_processor = document_processor\n",
    "        self.metrics = {\n",
    "            'total_documents': 0,\n",
    "            'total_chunks': 0,\n",
    "            'queries_processed': 0\n",
    "        }\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vector_store.initialize_index()\n",
    "        \n",
    "    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts using OpenAI's embedding model\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            List of embedding vectors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use OpenAI's text-embedding-ada-002 model\n",
    "            response = self.openai_client.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=texts\n",
    "            )\n",
    "            \n",
    "            embeddings = [embedding.embedding for embedding in response.data]\n",
    "            logger.info(f\"Generated {len(embeddings)} embeddings\")\n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def add_document(self, content: str, source: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a document to the knowledge base\n",
    "        \n",
    "        Args:\n",
    "            content: Document content\n",
    "            source: Document source/filename\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Process document into chunks\n",
    "            chunks = self.document_processor.process_document(content, source)\n",
    "            \n",
    "            if not chunks:\n",
    "                logger.warning(f\"No chunks generated for document: {source}\")\n",
    "                return\n",
    "            \n",
    "            # Generate embeddings for chunks\n",
    "            chunk_texts = [chunk['content'] for chunk in chunks]\n",
    "            embeddings = self.generate_embeddings(chunk_texts)\n",
    "            \n",
    "            # Store in vector database\n",
    "            vectors = []\n",
    "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                vector_id = f\"{source}_{i}\"\n",
    "                vectors.append({\n",
    "                    'id': vector_id,\n",
    "                    'values': embedding,\n",
    "                    'metadata': {\n",
    "                        'content': chunk['content'],\n",
    "                        'source': source,\n",
    "                        'chunk_index': i,\n",
    "                        'start_char': chunk['start_char'],\n",
    "                        'end_char': chunk['end_char']\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            self.vector_store.upsert_vectors(vectors)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics['total_documents'] += 1\n",
    "            self.metrics['total_chunks'] += len(chunks)\n",
    "            \n",
    "            logger.info(f\"Successfully added document: {source} with {len(chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding document {source}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant document chunks for a query\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant chunks with metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate embedding for query\n",
    "            query_embedding = self.generate_embeddings([query])[0]\n",
    "            \n",
    "            # Search in vector store\n",
    "            results = self.vector_store.query_vectors(\n",
    "                query_embedding,\n",
    "                top_k=top_k,\n",
    "                include_metadata=True\n",
    "            )\n",
    "            \n",
    "            # Format results\n",
    "            relevant_chunks = []\n",
    "            for match in results['matches']:\n",
    "                relevant_chunks.append({\n",
    "                    'content': match['metadata']['content'],\n",
    "                    'source': match['metadata']['source'],\n",
    "                    'score': match['score'],\n",
    "                    'metadata': match['metadata']\n",
    "                })\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(relevant_chunks)} relevant chunks for query\")\n",
    "            return relevant_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving relevant chunks: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]], temperature: float = 0.1) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using retrieved context and OpenAI\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            context_chunks: Retrieved relevant chunks\n",
    "            temperature: Response temperature\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare context from retrieved chunks\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"Source: {chunk['source']}\\nContent: {chunk['content']}\"\n",
    "                for chunk in context_chunks\n",
    "            ])\n",
    "            \n",
    "            # Create prompt for answer generation\n",
    "            system_prompt = f\"\"\"You are a helpful business assistant that answers questions based on the provided business knowledge base. \n",
    "            \n",
    "            Instructions:\n",
    "            1. Use only the information provided in the context to answer questions\n",
    "            2. If the answer cannot be found in the context, clearly state that you don't have enough information\n",
    "            3. Be concise but comprehensive in your responses\n",
    "            4. Reference specific sources when possible\n",
    "            5. Focus on business-relevant information and practical advice\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {query}\n",
    "            \n",
    "            Please provide a helpful and accurate answer based on the context above.\"\"\"\n",
    "            \n",
    "            # the newest OpenAI model is \"gpt-4o\" which was released May 13, 2024.\n",
    "            # do not change this unless explicitly requested by the user\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt\n",
    "                    }\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            logger.info(\"Generated answer using OpenAI\")\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 5, temperature: float = 0.1) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main query method that retrieves relevant information and generates an answer\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            top_k: Number of top results to retrieve\n",
    "            temperature: Response temperature\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing answer and sources\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Retrieve relevant chunks\n",
    "            relevant_chunks = self.retrieve_relevant_chunks(query, top_k)\n",
    "            \n",
    "            if not relevant_chunks:\n",
    "                return {\n",
    "                    'answer': \"I don't have enough information in the knowledge base to answer your question.\",\n",
    "                    'sources': []\n",
    "                }\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = self.generate_answer(query, relevant_chunks, temperature)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics['queries_processed'] += 1\n",
    "            \n",
    "            return {\n",
    "                'answer': answer,\n",
    "                'sources': relevant_chunks\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {e}\")\n",
    "            return {\n",
    "                'answer': f\"Sorry, I encountered an error while processing your question: {str(e)}\",\n",
    "                'sources': []\n",
    "            }\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get system performance metrics\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        return self.metrics.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample-data"
   },
   "source": [
    "## Sample Business Documents\n",
    "\n",
    "Let's create some sample business documents to demonstrate the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample-documents"
   },
   "outputs": [],
   "source": [
    "# Sample business documents\n",
    "business_policy = \"\"\"\n",
    "ACME Corporation Business Policy Manual\n",
    "\n",
    "1. CODE OF CONDUCT\n",
    "\n",
    "1.1 Professional Behavior\n",
    "All employees must maintain professional conduct at all times. This includes:\n",
    "- Treating colleagues, customers, and partners with respect and dignity\n",
    "- Maintaining confidentiality of sensitive company information\n",
    "- Avoiding conflicts of interest\n",
    "- Reporting any unethical behavior to management\n",
    "\n",
    "1.2 Dress Code\n",
    "Business casual attire is required for all office employees. Remote employees should dress professionally for video calls and client meetings.\n",
    "\n",
    "1.3 Communication Standards\n",
    "- All business communications must be professional and courteous\n",
    "- Use company email for business purposes only\n",
    "- Social media posts should not reference company matters without approval\n",
    "\n",
    "2. HUMAN RESOURCES POLICIES\n",
    "\n",
    "2.1 Equal Employment Opportunity\n",
    "ACME Corporation is an equal opportunity employer. We do not discriminate based on race, color, religion, gender, national origin, age, disability, or sexual orientation.\n",
    "\n",
    "2.2 Harassment Prevention\n",
    "We maintain a zero-tolerance policy for harassment of any kind. All employees have the right to work in an environment free from harassment, intimidation, and offensive behavior.\n",
    "\n",
    "2.3 Performance Reviews\n",
    "Annual performance reviews are conducted for all employees. Reviews assess job performance, goal achievement, and professional development needs.\n",
    "\n",
    "3. WORKPLACE SAFETY\n",
    "\n",
    "3.1 General Safety\n",
    "- Report all accidents and injuries immediately\n",
    "- Follow all safety protocols and procedures\n",
    "- Use appropriate personal protective equipment when required\n",
    "- Maintain clean and organized workspaces\n",
    "\n",
    "3.2 Emergency Procedures\n",
    "In case of emergency:\n",
    "- Evacuate the building using designated exit routes\n",
    "- Report to the designated assembly area\n",
    "- Do not use elevators during emergencies\n",
    "- Follow instructions from emergency personnel\n",
    "\n",
    "4. INFORMATION TECHNOLOGY\n",
    "\n",
    "4.1 Computer and Network Usage\n",
    "- Use company computers and networks for business purposes only\n",
    "- Do not install unauthorized software\n",
    "- Report security incidents immediately\n",
    "- Follow password security best practices\n",
    "\n",
    "4.2 Data Protection\n",
    "- Protect confidential and proprietary information\n",
    "- Use secure methods for data transmission\n",
    "- Back up important data regularly\n",
    "- Comply with data privacy regulations\n",
    "\n",
    "5. FINANCIAL POLICIES\n",
    "\n",
    "5.1 Expense Reports\n",
    "All business expenses must be documented and submitted within 30 days. Receipts are required for all expenses over $25.\n",
    "\n",
    "5.2 Procurement\n",
    "All purchases over $1,000 require management approval. Use approved vendors when possible.\n",
    "\n",
    "5.3 Travel Policy\n",
    "Business travel must be pre-approved by direct supervisor. Use cost-effective transportation and accommodation options.\n",
    "\n",
    "6. DISCIPLINARY PROCEDURES\n",
    "\n",
    "6.1 Progressive Discipline\n",
    "ACME Corporation follows a progressive discipline policy:\n",
    "1. Verbal warning\n",
    "2. Written warning\n",
    "3. Suspension\n",
    "4. Termination\n",
    "\n",
    "6.2 Termination\n",
    "Employment may be terminated for cause, including but not limited to:\n",
    "- Violation of company policies\n",
    "- Poor performance\n",
    "- Misconduct\n",
    "- Attendance issues\n",
    "\n",
    "This policy manual is effective as of January 1, 2024, and supersedes all previous versions.\n",
    "\"\"\"\n",
    "\n",
    "employee_handbook = \"\"\"\n",
    "ACME Corporation Employee Handbook\n",
    "\n",
    "Welcome to ACME Corporation! This handbook provides essential information about our company policies, procedures, and benefits.\n",
    "\n",
    "COMPANY OVERVIEW\n",
    "\n",
    "Mission Statement\n",
    "To provide innovative solutions that drive business success while maintaining the highest standards of integrity and customer service.\n",
    "\n",
    "Core Values\n",
    "- Innovation: We constantly seek new and better ways to serve our customers\n",
    "- Integrity: We conduct business ethically and transparently\n",
    "- Excellence: We strive for the highest quality in everything we do\n",
    "- Teamwork: We work together to achieve common goals\n",
    "\n",
    "EMPLOYMENT BASICS\n",
    "\n",
    "Work Hours\n",
    "- Standard business hours: 9:00 AM to 5:00 PM, Monday through Friday\n",
    "- Lunch break: 1 hour between 12:00 PM and 2:00 PM\n",
    "- Flexible work arrangements available with supervisor approval\n",
    "\n",
    "Attendance Policy\n",
    "- Regular attendance is essential for business operations\n",
    "- Notify supervisor as soon as possible if unable to work\n",
    "- Excessive absenteeism may result in disciplinary action\n",
    "\n",
    "Time Off Policies\n",
    "\n",
    "Vacation Time\n",
    "- New employees: 10 days annually\n",
    "- 2-5 years of service: 15 days annually\n",
    "- 5+ years of service: 20 days annually\n",
    "- Vacation requests must be submitted at least 2 weeks in advance\n",
    "\n",
    "Sick Leave\n",
    "- All employees receive 5 sick days annually\n",
    "- Sick leave does not roll over to the next year\n",
    "- Medical documentation may be required for extended absences\n",
    "\n",
    "Personal Days\n",
    "- 3 personal days per year for personal matters\n",
    "- Cannot be used in conjunction with vacation time\n",
    "- Must be approved by supervisor\n",
    "\n",
    "Holidays\n",
    "ACME Corporation observes the following holidays:\n",
    "- New Year's Day\n",
    "- Memorial Day\n",
    "- Independence Day\n",
    "- Labor Day\n",
    "- Thanksgiving Day\n",
    "- Christmas Day\n",
    "\n",
    "BENEFITS\n",
    "\n",
    "Health Insurance\n",
    "- Company pays 80% of premium for employee coverage\n",
    "- Family coverage available with employee contribution\n",
    "- Enrollment period: First 30 days of employment\n",
    "\n",
    "Retirement Plan\n",
    "- 401(k) plan with company matching up to 4% of salary\n",
    "- Immediate vesting for employee contributions\n",
    "- Company match vests after 3 years of service\n",
    "\n",
    "Life Insurance\n",
    "- Company-provided life insurance equal to 2x annual salary\n",
    "- Additional voluntary life insurance available for purchase\n",
    "\n",
    "Disability Insurance\n",
    "- Short-term disability: 60% of salary for up to 26 weeks\n",
    "- Long-term disability: 60% of salary after 26 weeks\n",
    "\n",
    "PROFESSIONAL DEVELOPMENT\n",
    "\n",
    "Training Opportunities\n",
    "- Annual training budget of $2,000 per employee\n",
    "- Professional conference attendance encouraged\n",
    "- Internal training programs available\n",
    "\n",
    "Tuition Reimbursement\n",
    "- Up to $5,000 annually for job-related education\n",
    "- Minimum grade of \"B\" required for reimbursement\n",
    "- Pre-approval required from HR and supervisor\n",
    "\n",
    "Career Development\n",
    "- Annual career development discussions\n",
    "- Mentorship program available\n",
    "- Internal job posting priority for existing employees\n",
    "\n",
    "WORKPLACE POLICIES\n",
    "\n",
    "Remote Work Policy\n",
    "- Remote work available for eligible positions\n",
    "- Requires supervisor approval and signed agreement\n",
    "- Home office equipment provided by company\n",
    "\n",
    "Technology Use\n",
    "- Company equipment for business use only\n",
    "- No personal software installation without approval\n",
    "- Regular security updates required\n",
    "\n",
    "Communication Guidelines\n",
    "- Professional email etiquette required\n",
    "- Confidential information must be protected\n",
    "- Social media policy applies to all platforms\n",
    "\n",
    "COMPENSATION\n",
    "\n",
    "Payroll\n",
    "- Bi-weekly pay schedule\n",
    "- Direct deposit required\n",
    "- Pay stubs available through employee portal\n",
    "\n",
    "Performance Reviews\n",
    "- Annual performance evaluations\n",
    "- Merit increases based on performance and budget\n",
    "- Promotion opportunities posted internally first\n",
    "\n",
    "Overtime Policy\n",
    "- Non-exempt employees eligible for overtime pay\n",
    "- Overtime must be pre-approved by supervisor\n",
    "- Time and a half for hours worked over 40 per week\n",
    "\n",
    "EMPLOYEE RESOURCES\n",
    "\n",
    "Human Resources\n",
    "- HR office hours: 8:00 AM to 4:30 PM\n",
    "- Employee assistance program available\n",
    "- Confidential reporting hotline: 1-800-ETHICS\n",
    "\n",
    "Employee Recognition\n",
    "- Monthly employee spotlight program\n",
    "- Annual service awards\n",
    "- Peer recognition system\n",
    "\n",
    "Wellness Programs\n",
    "- On-site fitness facility\n",
    "- Wellness seminars and health screenings\n",
    "- Mental health resources available\n",
    "\n",
    "CONTACT INFORMATION\n",
    "\n",
    "Human Resources: hr@acmecorp.com | Extension 1001\n",
    "IT Support: support@acmecorp.com | Extension 2000\n",
    "Facilities: facilities@acmecorp.com | Extension 3000\n",
    "Employee Hotline: 1-800-ETHICS\n",
    "\n",
    "This handbook is effective as of January 1, 2024. Policies may be updated as needed with proper notice to employees.\n",
    "\"\"\"\n",
    "\n",
    "company_faq = \"\"\"\n",
    "ACME Corporation Frequently Asked Questions\n",
    "\n",
    "GENERAL COMPANY INFORMATION\n",
    "\n",
    "Q: When was ACME Corporation founded?\n",
    "A: ACME Corporation was founded in 1985 and has been serving customers for over 35 years.\n",
    "\n",
    "Q: What services does ACME Corporation provide?\n",
    "A: We provide comprehensive business solutions including consulting, software development, project management, and technical support services.\n",
    "\n",
    "Q: How many employees does ACME Corporation have?\n",
    "A: We currently have approximately 500 employees across our three office locations.\n",
    "\n",
    "Q: What are ACME Corporation's office locations?\n",
    "A: Our offices are located in New York City (headquarters), Chicago, and San Francisco.\n",
    "\n",
    "EMPLOYMENT QUESTIONS\n",
    "\n",
    "Q: How do I apply for a job at ACME Corporation?\n",
    "A: Job applications can be submitted through our careers page on the company website or by emailing your resume to careers@acmecorp.com.\n",
    "\n",
    "Q: What is the hiring process like?\n",
    "A: Our hiring process typically includes: initial application review, phone screening, in-person or video interview, skills assessment (if applicable), and reference checks.\n",
    "\n",
    "Q: Does ACME Corporation offer internships?\n",
    "A: Yes, we offer paid internships during summer and winter breaks. Applications are accepted from students in relevant degree programs.\n",
    "\n",
    "Q: What is the company culture like?\n",
    "A: ACME Corporation promotes a collaborative, innovative work environment that values work-life balance and professional growth.\n",
    "\n",
    "BENEFITS AND COMPENSATION\n",
    "\n",
    "Q: What benefits does ACME Corporation offer?\n",
    "A: We offer comprehensive benefits including health insurance, dental and vision coverage, 401(k) with company match, paid time off, life insurance, and professional development opportunities.\n",
    "\n",
    "Q: How often are performance reviews conducted?\n",
    "A: Performance reviews are conducted annually, with informal check-ins quarterly.\n",
    "\n",
    "Q: Are there opportunities for advancement?\n",
    "A: Yes, we prioritize internal promotions and provide career development resources to help employees advance their careers.\n",
    "\n",
    "Q: Does the company offer flexible work arrangements?\n",
    "A: Yes, we offer flexible work schedules and remote work options for eligible positions, subject to supervisor approval.\n",
    "\n",
    "WORKPLACE POLICIES\n",
    "\n",
    "Q: What is the dress code policy?\n",
    "A: We maintain a business casual dress code for office employees. Professional attire is required for client meetings and presentations.\n",
    "\n",
    "Q: Are personal devices allowed to be used for work?\n",
    "A: Personal devices may be used for work with IT approval and proper security measures installed.\n",
    "\n",
    "Q: What is the policy on overtime work?\n",
    "A: Non-exempt employees are eligible for overtime pay at time and a half for hours worked over 40 per week. All overtime must be pre-approved by supervisors.\n",
    "\n",
    "Q: How are vacation requests handled?\n",
    "A: Vacation requests should be submitted at least 2 weeks in advance through the employee portal or to your supervisor.\n",
    "\n",
    "TRAINING AND DEVELOPMENT\n",
    "\n",
    "Q: What training opportunities are available?\n",
    "A: We offer various training programs including technical skills development, leadership training, and professional certification support.\n",
    "\n",
    "Q: Does the company pay for professional development?\n",
    "A: Yes, each employee has an annual professional development budget of $2,000 for training, conferences, and certifications.\n",
    "\n",
    "Q: Is tuition reimbursement available?\n",
    "A: Yes, we offer up to $5,000 annually in tuition reimbursement for job-related education with pre-approval.\n",
    "\n",
    "TECHNOLOGY AND EQUIPMENT\n",
    "\n",
    "Q: What equipment is provided to employees?\n",
    "A: All employees receive necessary equipment including laptop, monitor, keyboard, mouse, and software licenses required for their role.\n",
    "\n",
    "Q: How do I request IT support?\n",
    "A: IT support can be reached at support@acmecorp.com or extension 2000. For urgent issues, use the emergency IT hotline.\n",
    "\n",
    "Q: Are there restrictions on software installation?\n",
    "A: Yes, all software installations must be approved by IT to ensure security and compliance standards.\n",
    "\n",
    "HEALTH AND SAFETY\n",
    "\n",
    "Q: What safety protocols are in place?\n",
    "A: We maintain comprehensive safety protocols including emergency evacuation procedures, first aid stations, and safety training programs.\n",
    "\n",
    "Q: Are there wellness programs available?\n",
    "A: Yes, we offer on-site fitness facilities, wellness seminars, health screenings, and mental health resources.\n",
    "\n",
    "Q: How do I report a workplace injury?\n",
    "A: All workplace injuries must be reported immediately to your supervisor and HR. Incident reports must be filed within 24 hours.\n",
    "\n",
    "COMMUNICATION AND FEEDBACK\n",
    "\n",
    "Q: How can I provide feedback about company policies?\n",
    "A: Feedback can be provided through your supervisor, HR, or anonymously through our employee suggestion system.\n",
    "\n",
    "Q: Is there an open door policy?\n",
    "A: Yes, we maintain an open door policy where employees can discuss concerns with management at any level.\n",
    "\n",
    "Q: How does the company communicate important updates?\n",
    "A: Company updates are communicated through email, the employee portal, team meetings, and quarterly all-hands meetings.\n",
    "\n",
    "FACILITIES AND SERVICES\n",
    "\n",
    "Q: What facilities are available at the office?\n",
    "A: Our offices include conference rooms, break rooms, kitchen facilities, fitness center, and parking (where available).\n",
    "\n",
    "Q: Are there food services available?\n",
    "A: We provide complimentary coffee and snacks. Catered lunches are provided during company meetings and events.\n",
    "\n",
    "Q: How do I reserve conference rooms?\n",
    "A: Conference rooms can be reserved through the online booking system or by contacting facilities at extension 3000.\n",
    "\n",
    "CUSTOMER SERVICE\n",
    "\n",
    "Q: How can customers contact our support team?\n",
    "A: Customers can reach our support team through our website contact form, email at support@acmecorp.com, or by calling our main number.\n",
    "\n",
    "Q: What are our customer service hours?\n",
    "A: Customer service is available Monday through Friday, 8:00 AM to 6:00 PM EST.\n",
    "\n",
    "Q: How do we handle customer complaints?\n",
    "A: All customer complaints are taken seriously and are escalated to the appropriate management level for resolution.\n",
    "\n",
    "For additional questions not covered in this FAQ, please contact Human Resources at hr@acmecorp.com or extension 1001.\n",
    "\n",
    "Last updated: January 1, 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo-implementation"
   },
   "source": [
    "## Demo Implementation\n",
    "\n",
    "Let's initialize the RAG system and demonstrate its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize-system"
   },
   "outputs": [],
   "source": [
    "# Initialize the RAG system components\n",
    "print(\"Initializing RAG system...\")\n",
    "\n",
    "# Initialize components\n",
    "vector_store = VectorStore(PINECONE_API_KEY)\n",
    "document_processor = DocumentProcessor()\n",
    "rag_system = RAGSystem(OPENAI_API_KEY, vector_store, document_processor)\n",
    "\n",
    "print(\"RAG system initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "add-documents"
   },
   "outputs": [],
   "source": [
    "# Add sample documents to the knowledge base\n",
    "print(\"Adding sample documents to knowledge base...\")\n",
    "\n",
    "# Add business documents\n",
    "rag_system.add_document(business_policy, \"business_policy.txt\")\n",
    "rag_system.add_document(employee_handbook, \"employee_handbook.txt\")\n",
    "rag_system.add_document(company_faq, \"company_faq.txt\")\n",
    "\n",
    "print(\"Sample documents added successfully!\")\n",
    "\n",
    "# Display system metrics\n",
    "metrics = rag_system.get_metrics()\n",
    "print(f\"\\nSystem Metrics:\")\n",
    "print(f\"Total Documents: {metrics['total_documents']}\")\n",
    "print(f\"Total Chunks: {metrics['total_chunks']}\")\n",
    "print(f\"Queries Processed: {metrics['queries_processed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo-queries"
   },
   "source": [
    "## Demo Queries\n",
    "\n",
    "Let's test the RAG system with various business-related queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-queries"
   },
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the company's vacation policy?\",\n",
    "    \"What are the dress code requirements?\",\n",
    "    \"How do I report a workplace injury?\",\n",
    "    \"What benefits does the company offer?\",\n",
    "    \"What is the process for expense reports?\",\n",
    "    \"How can I apply for a job at ACME Corporation?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG system with sample queries...\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"=== Query {i}: {query} ===\")\n",
    "    \n",
    "    # Process query\n",
    "    result = rag_system.query(query, top_k=3)\n",
    "    \n",
    "    # Display answer\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    \n",
    "    # Display sources\n",
    "    print(f\"\\nSources:\")\n",
    "    for j, source in enumerate(result['sources'], 1):\n",
    "        print(f\"  {j}. {source['source']} (Score: {source['score']:.3f})\")\n",
    "        print(f\"     Content: {source['content'][:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive-demo"
   },
   "outputs": [],
   "source": [
    "# Interactive query interface\n",
    "def interactive_query():\n",
    "    \"\"\"\n",
    "    Interactive query interface for testing the RAG system\n",
    "    \"\"\"\n",
    "    print(\"Interactive RAG Query Interface\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter your question about ACME Corporation: \")\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not query.strip():\n",
    "            print(\"Please enter a valid question.\\n\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Process query\n",
    "            result = rag_system.query(query, top_k=3)\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\nAnswer: {result['answer']}\")\n",
    "            \n",
    "            if result['sources']:\n",
    "                print(f\"\\nSources:\")\n",
    "                for i, source in enumerate(result['sources'], 1):\n",
    "                    print(f\"  {i}. {source['source']} (Relevance: {source['score']:.3f})\")\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\\n\")\n",
    "\n",
    "# Run interactive demo\n",
    "interactive_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance-analysis"
   },
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance-metrics"
   },
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "import time\n",
    "\n",
    "def analyze_performance():\n",
    "    \"\"\"\n",
    "    Analyze RAG system performance with various queries\n",
    "    \"\"\"\n",
    "    performance_queries = [\n",
    "        \"What is the company mission?\",\n",
    "        \"How do I request time off?\",\n",
    "        \"What are the IT policies?\",\n",
    "        \"Tell me about employee benefits\",\n",
    "        \"What is the disciplinary procedure?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Performance Analysis\\n\")\n",
    "    print(f\"{'Query':<40} {'Time (s)':<10} {'Sources':<10} {'Answer Length':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    total_time = 0\n",
    "    total_queries = len(performance_queries)\n",
    "    \n",
    "    for query in performance_queries:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = rag_system.query(query, top_k=3)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        query_time = end_time - start_time\n",
    "        total_time += query_time\n",
    "        \n",
    "        # Truncate query for display\n",
    "        display_query = query if len(query) <= 37 else query[:37] + \"...\"\n",
    "        \n",
    "        print(f\"{display_query:<40} {query_time:<10.2f} {len(result['sources']):<10} {len(result['answer']):<15}\")\n",
    "    \n",
    "    print(\"-\" * 75)\n",
    "    print(f\"Average query time: {total_time/total_queries:.2f} seconds\")\n",
    "    print(f\"Total queries processed: {rag_system.get_metrics()['queries_processed']}\")\n",
    "    \n",
    "    return total_time / total_queries\n",
    "\n",
    "# Run performance analysis\n",
    "avg_time = analyze_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a complete RAG implementation for business QA systems. The system includes:\n",
    "\n",
    "### Key Features:\n",
    "1. **Document Processing**: Intelligent text cleaning and chunking\n",
    "2. **Vector Storage**: Efficient semantic search using Pinecone\n",
    "3. **Embedding Generation**: OpenAI's text-embedding-ada-002 model\n",
    "4. **Answer Generation**: GPT-4o for contextual responses\n",
    "5. **Source Attribution**: Transparent sourcing of information\n",
    "\n",
    "### System Performance:\n",
    "- **Average Query Time**: Fast response times for business queries\n",
    "- **Scalability**: Handles multiple documents and concurrent queries\n",
    "- **Accuracy**: Contextually relevant answers with source attribution\n",
    "\n",
    "### Business Applications:\n",
    "- Employee onboarding and training\n",
    "- HR policy inquiries\n",
    "- Compliance and procedure questions\n",
    "- Customer service automation\n",
    "- Knowledge management\n",
    "\n",
    "### Next Steps:\n",
    "1. Deploy as a web application using Streamlit\n",
    "2. Add support for multiple file formats (PDF, DOCX)\n",
    "3. Implement user authentication and access control\n",
    "4. Add analytics and usage tracking\n",
    "5. Optimize for production deployment\n",
    "\n",
    "This RAG system provides a solid foundation for building enterprise-grade business QA applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}